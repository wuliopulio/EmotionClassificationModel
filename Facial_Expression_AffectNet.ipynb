{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wuliopulio/EmotionClassificationModel/blob/main/Facial_Expression_AffectNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BziFRPUkohVB"
      },
      "source": [
        "# Transfer Learning with AffectNet\n",
        "https://www.kaggle.com/datasets/mstjebashazida/affectnet/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXNN8BThrBio"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2Z5EXAuKldG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8F09eSIKx8H",
        "outputId": "4d83124f-9cb1-4c57-cbb7-ac8b5a3ad328"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX58aCCnqjpW"
      },
      "source": [
        "### 1. Load and Pre-process Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ybt8eitEj_9N",
        "outputId": "b35f63ab-721a-4d66-fa85-3eb4ff90cb5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /kaggle/input/affectnet\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "image_data_path = kagglehub.dataset_download(\"mstjebashazida/affectnet\")\n",
        "\n",
        "print(\"Path to dataset files:\", image_data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-FCrhObSlQO"
      },
      "outputs": [],
      "source": [
        "class AFFECTNETDatasetTrain(Dataset):\n",
        "  def __init__(self, data_dir, transform):\n",
        "    super(AFFECTNETDatasetTrain).__init__()\n",
        "    self.labels = os.listdir(data_dir)\n",
        "    self.transform = transform\n",
        "\n",
        "    classes = sorted(os.listdir(image_data_path))\n",
        "\n",
        "    class_to_index = {}\n",
        "\n",
        "    self.samples = []\n",
        "\n",
        "    class_to_index = {\n",
        "        \"anger\" : 2,\n",
        "        \"contempt\" : 2,\n",
        "        \"disgust\" : 5,\n",
        "        \"fear\" : 1,\n",
        "        \"happy\" : 6,\n",
        "        \"neutral\" : 3,\n",
        "        \"sad\" : 4,\n",
        "        \"surprise\" : 0\n",
        "    }\n",
        "\n",
        "    for label in self.labels:\n",
        "       for image_path in os.listdir(os.path.join(data_dir, label)):\n",
        "          fpath = os.path.join(data_dir, label, image_path)\n",
        "          label_index = class_to_index[label]\n",
        "          self.samples.append((fpath, label_index))\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.samples)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      image_path, label = self.samples[idx]\n",
        "      image = Image.open(image_path)\n",
        "      if self.transform is not None:\n",
        "        image = self.transform(image)\n",
        "      return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "768blm5rn5qa"
      },
      "outputs": [],
      "source": [
        "class AFFECTNETDatasetTest(Dataset):\n",
        "  def __init__(self, data_dir, transform):\n",
        "    super(AFFECTNETDatasetTest).__init__()\n",
        "    self.labels = os.listdir(data_dir)\n",
        "    self.transform = transform\n",
        "\n",
        "    classes = sorted(os.listdir(image_data_path))\n",
        "\n",
        "    class_to_index = {}\n",
        "\n",
        "    self.samples = []\n",
        "\n",
        "    class_to_index = {\n",
        "        \"Anger\" : 2,\n",
        "        \"Contempt\" : 2,\n",
        "        \"disgust\" : 5,\n",
        "        \"fear\" : 1,\n",
        "        \"happy\" : 6,\n",
        "        \"neutral\" : 3,\n",
        "        \"sad\" : 4,\n",
        "        \"surprise\" : 0\n",
        "    }\n",
        "\n",
        "    for label in self.labels:\n",
        "       for image_path in os.listdir(os.path.join(data_dir, label)):\n",
        "          fpath = os.path.join(data_dir, label, image_path)\n",
        "          label_index = class_to_index[label]\n",
        "          self.samples.append((fpath, label_index))\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.samples)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      image_path, label = self.samples[idx]\n",
        "      image = Image.open(image_path)\n",
        "      if self.transform is not None:\n",
        "        image = self.transform(image)\n",
        "      return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9kl6AlucAO2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((48, 48)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(45),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Grayscale(),\n",
        "\t  transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "     transforms.Resize((48, 48)),\n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "trainset = AFFECTNETDatasetTrain(os.path.join(image_data_path, 'archive (3)', 'Train'), train_transform)\n",
        "testset = AFFECTNETDatasetTest(os.path.join(image_data_path, 'archive (3)', 'Test'), test_transform)\n",
        "\n",
        "# Loaders\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2UbWTGn6SX0"
      },
      "outputs": [],
      "source": [
        "class EmotionCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(EmotionCNN, self).__init__()\n",
        "\n",
        "    # Define layers\n",
        "\n",
        "    # Block 1:  48*48*3 -> 48 * 48 * 16 -> 24*24*16\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    # Block 2: 24 * 24 * 16 -> 24 * 24 * 32 -> 12 * 12 * 32\n",
        "    self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    # Block 3: 12 * 12 * 32 -> 12 * 12 * 64 -> 6 * 6 * 64\n",
        "    self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "    self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    # Block 4: 6 * 6 * 64 -> 6 * 6 * 128 -> 3 * 3 * 128\n",
        "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "    self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    # FC layers for classification\n",
        "    self.fc1 = nn.Linear(128*3*3, 512)\n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "    self.fc2 = nn.Linear(512, 256)\n",
        "    self.fc3 = nn.Linear(256, 7)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # CONV -> RELU -> POOL\n",
        "\n",
        "    # Block 1\n",
        "    x = self.pool1(F.relu(self.conv1(x)))\n",
        "    # Block 2\n",
        "    x = self.pool2(F.relu(self.conv2(x)))\n",
        "    # Block 3\n",
        "    x = self.pool3(F.relu(self.conv3(x)))\n",
        "    # Block 4\n",
        "    x = self.pool4(F.relu(self.conv4(x)))\n",
        "\n",
        "    # Flatten x\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    # MLP\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AN5nxxkBsaIb",
        "outputId": "74fe5e95-448b-4772-8c8b-19f50593aa93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g3GPUORl4RB",
        "outputId": "4ae628d7-b8e9-42b7-d614-defe67f1105d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Correct full path\n",
        "model_path = '/content/drive/MyDrive/Emotion Classifier CNN/Models/emotion_facial_model_1.pth'\n",
        "\n",
        "# Load weights\n",
        "model = EmotionCNN().to(device)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-ZYBnaB_P5G"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, num_epochs, model_name='Model'):\n",
        "  train_losses = []\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  print(f\"---Training {model_name} for {num_epochs} epochs---\")\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc = f'Epoch {epoch + 1}/{num_epochs}') # displays a progress bar for the training\n",
        "    for data, target in pbar:\n",
        "      data, target = data.to(device), target.to(device) # moves both to the same device\n",
        "\n",
        "      #Step 1. zero the gradient\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      #Step 2. Forward model\n",
        "      output = model(data)\n",
        "\n",
        "      # Step 3. Calculate the loss\n",
        "      loss = criterion(output, target)\n",
        "\n",
        "      # Step 4: Backward Pass\n",
        "      loss.backward()\n",
        "\n",
        "      # Step 5: Update the weights\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      _, predicted = torch.max(output.data, 1) # Gets the predicted class with the highest probability for each example.\n",
        "\n",
        "      #Updates the total number of samples and correct predictions.\n",
        "      total += target.size(0)\n",
        "      correct += (predicted == target).sum().item()\n",
        "\n",
        "      pbar.set_postfix({'Loss': f'{running_loss/(pbar.n+1):.4f}', 'Acc': f'{100.*correct/total:.2f}%'})\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100. * correct/total #Computes the average loss and accuracy for the full epoch.\n",
        "    train_losses.append(epoch_loss)\n",
        "\n",
        "  print(f'--- Finished training {model_name} ---')\n",
        "  return train_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFyX73vL_TG2"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model,test_loader):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      _, predicted = torch.max(output.data, 1 )\n",
        "      total += target.size(0)\n",
        "      correct += (predicted == target).sum().item()\n",
        "\n",
        "  return 100. * correct/total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlrCZX7U_VQ5",
        "outputId": "f7003789-f815-40fc-d7a0-56774558356d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---Training Emotions CNN for 30 epochs---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/30: 100%|██████████| 504/504 [01:05<00:00,  7.70it/s, Loss=1.5451, Acc=39.31%]\n",
            "Epoch 2/30: 100%|██████████| 504/504 [00:23<00:00, 21.24it/s, Loss=1.3198, Acc=47.12%]\n",
            "Epoch 3/30: 100%|██████████| 504/504 [00:23<00:00, 21.30it/s, Loss=1.2498, Acc=49.76%]\n",
            "Epoch 4/30: 100%|██████████| 504/504 [00:23<00:00, 21.12it/s, Loss=1.2158, Acc=50.92%]\n",
            "Epoch 5/30: 100%|██████████| 504/504 [00:23<00:00, 21.19it/s, Loss=1.1851, Acc=52.58%]\n",
            "Epoch 6/30: 100%|██████████| 504/504 [00:23<00:00, 21.18it/s, Loss=1.1663, Acc=53.27%]\n",
            "Epoch 7/30: 100%|██████████| 504/504 [00:23<00:00, 21.66it/s, Loss=1.1573, Acc=53.43%]\n",
            "Epoch 8/30: 100%|██████████| 504/504 [00:23<00:00, 21.64it/s, Loss=1.1469, Acc=53.56%]\n",
            "Epoch 9/30: 100%|██████████| 504/504 [00:23<00:00, 21.39it/s, Loss=1.1241, Acc=54.90%]\n",
            "Epoch 10/30: 100%|██████████| 504/504 [00:23<00:00, 21.54it/s, Loss=1.1212, Acc=54.92%]\n",
            "Epoch 11/30: 100%|██████████| 504/504 [00:23<00:00, 21.56it/s, Loss=1.1077, Acc=54.95%]\n",
            "Epoch 12/30: 100%|██████████| 504/504 [00:23<00:00, 21.39it/s, Loss=1.1033, Acc=55.74%]\n",
            "Epoch 13/30: 100%|██████████| 504/504 [00:23<00:00, 21.10it/s, Loss=1.1042, Acc=55.62%]\n",
            "Epoch 14/30: 100%|██████████| 504/504 [00:24<00:00, 20.97it/s, Loss=1.0879, Acc=56.02%]\n",
            "Epoch 15/30: 100%|██████████| 504/504 [00:23<00:00, 21.13it/s, Loss=1.0920, Acc=56.67%]\n",
            "Epoch 16/30: 100%|██████████| 504/504 [00:23<00:00, 21.47it/s, Loss=1.0743, Acc=56.74%]\n",
            "Epoch 17/30: 100%|██████████| 504/504 [00:24<00:00, 20.92it/s, Loss=1.0638, Acc=57.58%]\n",
            "Epoch 18/30: 100%|██████████| 504/504 [00:23<00:00, 21.49it/s, Loss=1.0718, Acc=57.58%]\n",
            "Epoch 19/30: 100%|██████████| 504/504 [00:23<00:00, 21.86it/s, Loss=1.0649, Acc=57.55%]\n",
            "Epoch 20/30: 100%|██████████| 504/504 [00:23<00:00, 21.31it/s, Loss=1.0636, Acc=57.65%]\n",
            "Epoch 21/30: 100%|██████████| 504/504 [00:23<00:00, 21.82it/s, Loss=1.0619, Acc=57.87%]\n",
            "Epoch 22/30: 100%|██████████| 504/504 [00:24<00:00, 20.80it/s, Loss=1.0543, Acc=58.06%]\n",
            "Epoch 23/30: 100%|██████████| 504/504 [00:24<00:00, 20.95it/s, Loss=1.0505, Acc=58.24%]\n",
            "Epoch 24/30: 100%|██████████| 504/504 [00:24<00:00, 20.62it/s, Loss=1.0494, Acc=58.54%]\n",
            "Epoch 25/30: 100%|██████████| 504/504 [00:24<00:00, 20.84it/s, Loss=1.0504, Acc=58.13%]\n",
            "Epoch 26/30: 100%|██████████| 504/504 [00:23<00:00, 21.41it/s, Loss=1.0424, Acc=58.80%]\n",
            "Epoch 27/30: 100%|██████████| 504/504 [00:24<00:00, 20.88it/s, Loss=1.0341, Acc=59.16%]\n",
            "Epoch 28/30: 100%|██████████| 504/504 [00:24<00:00, 20.93it/s, Loss=1.0308, Acc=58.90%]\n",
            "Epoch 29/30: 100%|██████████| 504/504 [00:24<00:00, 20.97it/s, Loss=1.0319, Acc=58.73%]\n",
            "Epoch 30/30: 100%|██████████| 504/504 [00:24<00:00, 20.80it/s, Loss=1.0406, Acc=58.87%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Finished training Emotions CNN ---\n",
            "Accuracy of Emotions CNN:57.29%\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_cnn = optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "cnn_losses = train_model(model, trainloader, criterion, optimizer_cnn, num_epochs= 30, model_name = 'Emotions CNN')\n",
        "cnn_accuracy = evaluate_model(model, testloader)\n",
        "print(f'Accuracy of Emotions CNN:{cnn_accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i97x3jJl_Yb_",
        "outputId": "557f9dfb-049e-4176-e1e4-af13ef59fd9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy (CNN): 57.29%\n",
            "Number of parameters (CNN): 820615\n"
          ]
        }
      ],
      "source": [
        "# Get numbers of parameters\n",
        "cnn_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f'Test accuracy (CNN): {cnn_accuracy:.2f}%')\n",
        "\n",
        "print(f'Number of parameters (CNN): {cnn_params}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6Ugt-OUq5Pm"
      },
      "source": [
        "### 4. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A8on1Yy1NrAm"
      },
      "outputs": [],
      "source": [
        "save_to = os.path.join('drive', 'MyDrive', 'Emotion Classifier CNN', 'Models', \"emotion_facial_model_2.pth\")\n",
        "os.makedirs(os.path.dirname(save_to), exist_ok=True)\n",
        "torch.save(model.state_dict(), save_to)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}